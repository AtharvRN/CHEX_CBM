{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5e70e7",
   "metadata": {},
   "source": [
    "# VLG-CBM CBL Analysis Notebook\n",
    "Use this notebook to load a trained VLG-CBM run, visualize the learned Concept Bottleneck Layer (CBL), and reproduce the experiments from Sections 5.2â€“5.4 of the paper (top activations, decision interpretability, and NEC pruning). Update `RUN_DIR` below to point to your model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab96e59",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Load the saved artifacts (`concept_layer.pt`, `W_g.pt`, etc.)\n",
    "2. Recompute concept activations on the validation split and cache image paths\n",
    "3. Plot the top-5 activated images per concept as in Figure 4\n",
    "4. Show the top contributions for a sample decision and compare predictions after pruning to NEC=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26279c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from dataset import CheXpertDataset, get_transforms\n",
    "from vlg_cbm_lib.datasets import BackboneWithConcepts, ConceptLayer\n",
    "from vlg_cbm_lib.eval import evaluate, evaluate_baseline_model\n",
    "from models import get_model, XRV_WEIGHTS\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_backbone_from_config(config, labels, device):\n",
    "    use_xrv_backbone = config[\"backbone\"] in XRV_WEIGHTS\n",
    "    backbone_kwargs = {}\n",
    "    if use_xrv_backbone:\n",
    "        backbone_kwargs[\"target_labels\"] = labels\n",
    "    backbone_model = get_model(\n",
    "        config[\"backbone\"],\n",
    "        num_classes=len(labels),\n",
    "        pretrained=True,\n",
    "        **backbone_kwargs,\n",
    "    )\n",
    "    if config.get(\"backbone_ckpt\"):\n",
    "        import inspect\n",
    "        load_kwargs = {\"map_location\": device}\n",
    "        if \"weights_only\" in inspect.signature(torch.load).parameters:\n",
    "            load_kwargs[\"weights_only\"] = False\n",
    "        ckpt = torch.load(config[\"backbone_ckpt\"], **load_kwargs)\n",
    "        state = ckpt.get(\"model_state_dict\", ckpt)\n",
    "        backbone_model.load_state_dict(state, strict=False)\n",
    "    if config[\"backbone\"] == \"densenet121\":\n",
    "        feature_dim = 1024\n",
    "        backbone = backbone_model.backbone.features\n",
    "    elif config[\"backbone\"] == \"resnet50\":\n",
    "        feature_dim = 2048\n",
    "        backbone = torch.nn.Sequential(*list(backbone_model.backbone.children())[:-1])\n",
    "    else:\n",
    "        feature_dim = getattr(backbone_model, \"feature_dim\", 1024)\n",
    "        class XRVDenseNetBackbone(torch.nn.Module):\n",
    "            def __init__(self, wrapper):\n",
    "                super().__init__()\n",
    "                self.wrapper = wrapper\n",
    "\n",
    "            def forward(self, x):\n",
    "                return self.wrapper.get_features(x)\n",
    "\n",
    "        backbone = XRVDenseNetBackbone(backbone_model)\n",
    "    return backbone_model, backbone.to(device), feature_dim\n",
    "\n",
    "class IndexedCheXpertDataset(CheXpertDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = super().__getitem__(idx)\n",
    "        return image, label, torch.tensor(idx, dtype=torch.long)\n",
    "\n",
    "def load_run_artifacts(run_dir):\n",
    "    run_dir = Path(run_dir)\n",
    "    config = json.loads(run_dir.joinpath(\"config.json\").read_text())\n",
    "    config[\"output\"] = str(run_dir)\n",
    "    concepts = [line.strip() for line in run_dir.joinpath(\"concepts.txt\").read_text().splitlines() if line.strip()]\n",
    "    labels = config[\"labels\"]\n",
    "    backbone_model, backbone, feature_dim = load_backbone_from_config(config, labels, DEVICE)\n",
    "    concept_layer = ConceptLayer(feature_dim, len(concepts), num_hidden=config.get(\"cbl_hidden_layers\", 1))\n",
    "    ckpt_path = run_dir / \"concept_layer_best.pt\"\n",
    "    if not ckpt_path.exists():\n",
    "        ckpt_path = run_dir / \"concept_layer.pt\"\n",
    "    concept_layer.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "    model = BackboneWithConcepts(backbone, concept_layer).to(DEVICE)\n",
    "    model.eval()\n",
    "    W = torch.load(run_dir / \"W_g.pt\")\n",
    "    b = torch.load(run_dir / \"b_g.pt\")\n",
    "    mean = torch.load(run_dir / \"concept_mean.pt\")\n",
    "    std = torch.load(run_dir / \"concept_std.pt\")\n",
    "    return config, concepts, labels, model, W, b, mean, std, backbone_model\n",
    "\n",
    "def make_dataset(config, split=\"valid\", split_csv=None):\n",
    "    data_dir = Path(config[\"data_dir\"])\n",
    "    if split_csv is not None:\n",
    "        csv_path = Path(split_csv)\n",
    "    else:\n",
    "        csv_path = data_dir / f\"{split}.csv\"\n",
    "    img_root = data_dir.parent\n",
    "    dataset = IndexedCheXpertDataset(\n",
    "        csv_path=str(csv_path),\n",
    "        img_root=str(img_root),\n",
    "        transform=get_transforms(224, is_training=False),\n",
    "        labels=config[\"labels\"],\n",
    "        uncertain_strategy=config.get(\"uncertain_strategy\", \"ones\"),\n",
    "        frontal_only=config.get(\"frontal_only\", True)\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def make_loader(dataset, batch_size=64):\n",
    "    def collate(batch):\n",
    "        images, labels, idxs = zip(*batch)\n",
    "        return torch.stack(images), torch.stack(labels), torch.stack(idxs)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate)\n",
    "\n",
    "def compute_activations(model, loader):\n",
    "    model.eval()\n",
    "    all_concepts = []\n",
    "    all_labels = []\n",
    "    all_idxs = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels_batch, idxs in loader:\n",
    "            images = images.to(DEVICE)\n",
    "            logits = model(images).cpu()\n",
    "            all_concepts.append(logits)\n",
    "            all_labels.append(labels_batch)\n",
    "            all_idxs.append(idxs)\n",
    "    return torch.cat(all_concepts), torch.cat(all_labels), torch.cat(all_idxs)\n",
    "\n",
    "def normalize_concepts(concepts, mean, std):\n",
    "    return (concepts - mean) / torch.clamp(std, min=1e-6)\n",
    "\n",
    "def final_logits(concepts, mean, std, W, b):\n",
    "    c_norm = normalize_concepts(concepts, mean, std)\n",
    "    return c_norm @ W.t() + b\n",
    "\n",
    "def compute_contributions(concepts, W):\n",
    "    probs = torch.sigmoid(concepts)\n",
    "    return probs.unsqueeze(1) * W.unsqueeze(0)\n",
    "\n",
    "def prune_weights(W, topk=5):\n",
    "    pruned = torch.zeros_like(W)\n",
    "    abs_W = W.abs()\n",
    "    for class_idx in range(W.size(0)):\n",
    "        topk_idxs = torch.topk(abs_W[class_idx], topk).indices\n",
    "        pruned[class_idx, topk_idxs] = W[class_idx, topk_idxs]\n",
    "    return pruned\n",
    "\n",
    "def display_images(paths, titles=None, figsize=(14, 3)):\n",
    "    n = len(paths)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, path, title in zip(axes, paths, titles or [None] * n):\n",
    "        ax.imshow(Image.open(path).convert(\"RGB\"))\n",
    "        ax.axis(\"off\")\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_top_images_for_concept(concept_idx, activations, idxs, dataset, concept_names, topk=5):\n",
    "    scores = activations[:, concept_idx]\n",
    "    top_indices = torch.argsort(scores, descending=True)[:topk]\n",
    "    paths = [dataset.get_image_path(int(idxs[i])) for i in top_indices]\n",
    "    titles = [f\"{concept_names[concept_idx]} ({scores[i].item():.3f})\" for i in top_indices]\n",
    "    display_images(paths, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DIR = Path(\"checkpoints/vlg_cbm_exp5\")  # update to your checkpoint directory\n",
    "config, concepts, labels, model, W, b, mean, std, backbone_model = load_run_artifacts(RUN_DIR)\n",
    "config['data_dir']=\"/workspace/datasets/test\"\n",
    "TEST_CSV = \"/workspace/datasets/test_labels.csv\"\n",
    "dataset = make_dataset(config, split=\"test\", split_csv=TEST_CSV)\n",
    "loader = make_loader(dataset, batch_size=64,)\n",
    "activations, pathology_labels, idxs = compute_activations(model, loader)\n",
    "logits = final_logits(activations, mean, std, W, b)\n",
    "probs = torch.sigmoid(logits)\n",
    "contributions = compute_contributions(activations, W)\n",
    "concept_predictions = torch.sigmoid(activations)\n",
    "print(\"Loaded run:\", RUN_DIR)\n",
    "print(\"Activation matrix shape:\", activations.shape)\n",
    "print(\"Dataset size:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating final model metrics and accuracy...')\n",
    "final_metrics = evaluate(activations, pathology_labels, W, b, mean, std, labels)\n",
    "print(f\"Final model mean AUROC: {final_metrics['mean_auroc']:.4f}, mean AP: {final_metrics['mean_ap']:.4f}\")\n",
    "final_preds = (probs >= 0.5).float()\n",
    "final_accuracy = (final_preds == pathology_labels).float().mean().item()\n",
    "print(f\"Final model (thresholded 0.5) accuracy: {final_accuracy:.4%}\")\n",
    "print('Evaluating backbone classifier...')\n",
    "baseline_dataset = CheXpertDataset(\n",
    "    csv_path=TEST_CSV,\n",
    "    img_root=str(Path(config['data_dir']).parent),\n",
    "    transform=get_transforms(224, is_training=False),\n",
    "    labels=config['labels'],\n",
    "    uncertain_strategy=config.get('uncertain_strategy', 'ones'),\n",
    "    frontal_only=config.get('frontal_only', True)\n",
    ")\n",
    "baseline_loader = DataLoader(baseline_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "baseline_model = backbone_model.to(DEVICE)\n",
    "baseline_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels_batch in baseline_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        logits = baseline_model(images)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).cpu()\n",
    "        correct += (preds == labels_batch).float().sum().item()\n",
    "        total += preds.numel()\n",
    "baseline_accuracy = correct / total\n",
    "print(f\"Backbone classifier accuracy (threshold=0.5): {baseline_accuracy:.4%}\")\n",
    "baseline_metrics = evaluate_baseline_model(baseline_model, baseline_loader, DEVICE, labels)\n",
    "print(f\"Backbone mean AUROC: {baseline_metrics['mean_auroc']:.4f}, mean AP: {baseline_metrics['mean_ap']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Per-class metrics (threshold=0.5):')\n",
    "binary_preds = final_preds\n",
    "per_class_stats = []\n",
    "for idx, label in enumerate(labels):\n",
    "    y_true = pathology_labels[:, idx].cpu().numpy()\n",
    "    y_pred = binary_preds[:, idx].cpu().numpy()\n",
    "    y_score = probs[:, idx].cpu().numpy()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = float('nan')\n",
    "    if np.unique(y_true).size > 1:\n",
    "        auc = roc_auc_score(y_true, y_score)\n",
    "    per_class_stats.append((label, acc, auc, prec, rec, f1))\n",
    "print(f\"{'Label':30} {'Acc':>6} {'AUC':>6} {'Prec':>6} {'Recall':>6} {'F1':>6}\")\n",
    "for label, acc, auc, prec, rec, f1 in per_class_stats:\n",
    "    auc_str = f\"{auc:.3f}\" if not np.isnan(auc) else 'n/a'\n",
    "    print(f\"{label:30} {acc:6.3f} {auc_str:>6} {prec:6.3f} {rec:6.3f} {f1:6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d5786",
   "metadata": {},
   "source": [
    "## Top-5 Activated Images per Concept\n",
    "Pick a concept of interest (e.g., one that matches VINDr tags) and inspect the five images with the highest CBL activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dcdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_idx = 0  # update with the concept you want to inspect\n",
    "plot_top_images_for_concept(concept_idx, activations, idxs, dataset, concepts, topk=5)\n",
    "print(\"Concept name:\", concepts[concept_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2097f",
   "metadata": {},
   "source": [
    "## Decision Interpretability Case Study\n",
    "Examine the top-5 concept contributions for a validation image by multiplying the concept prediction scores by `W_g`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_contributions(sample_idx, topk=5, prob_threshold=0.5):\n",
    "    sample_probs = probs[sample_idx]\n",
    "    sample_contribs = contributions[sample_idx]\n",
    "    positive_classes = torch.where(sample_probs >= prob_threshold)[0]\n",
    "    if positive_classes.numel() == 0:\n",
    "        positive_classes = torch.argsort(sample_probs, descending=True)[:1]\n",
    "    results = []\n",
    "    for class_idx in positive_classes:\n",
    "        class_contribs = sample_contribs[class_idx]\n",
    "        topk_idxs = torch.argsort(class_contribs.abs(), descending=True)[:topk]\n",
    "        results.append((\n",
    "            class_idx.item(),\n",
    "            sample_probs[class_idx].item(),\n",
    "            class_contribs[topk_idxs],\n",
    "            topk_idxs,\n",
    "        ))\n",
    "    return results\n",
    "\n",
    "sample_idx = 0  # change to another sample to inspect\n",
    "contrib_info = top_contributions(sample_idx, topk=5)\n",
    "print(\"Sample image:\", dataset.get_image_path(int(idxs[sample_idx])))\n",
    "display_images([dataset.get_image_path(int(idxs[sample_idx]))], [f\"Sample predictions\"], figsize=(5, 5))\n",
    "for class_idx, class_prob, contribs, contrib_idxs in contrib_info:\n",
    "    print(f\"Top concepts for {labels[class_idx]} (prob {class_prob:.3f}):\")\n",
    "    for rank, (idx, score) in enumerate(zip(contrib_idxs, contribs), 1):\n",
    "        print(f\"{rank}. {concepts[idx]} -> contribution {score.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621baaab",
   "metadata": {},
   "source": [
    "## Random Sample Contributions\n",
    "Pick a random validation image and visualize the concept contributions for the most confident predicted pathology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ae142",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = torch.randint(len(dataset), (1,)).item()\n",
    "random_path = dataset.get_image_path(int(idxs[random_idx]))\n",
    "random_probs = probs[random_idx]\n",
    "pred_class = int(torch.argmax(random_probs))\n",
    "prob_value = random_probs[pred_class].item()\n",
    "class_contribs = contributions[random_idx, pred_class]\n",
    "topk = 12\n",
    "concept_idxs = torch.argsort(class_contribs.abs(), descending=True)[:topk]\n",
    "concept_scores = class_contribs[concept_idxs]\n",
    "concept_names = [concepts[i] for i in concept_idxs]\n",
    "\n",
    "print(f\"Dataset index: {idxs[random_idx].item()} (random sample)\")\n",
    "print(\"Predicted class:\", labels[pred_class], f\"(prob {prob_value:.3f})\")\n",
    "truth_idxs = torch.where(pathology_labels[random_idx] == 1)[0]\n",
    "truth_names = [labels[i] for i in truth_idxs]\n",
    "print(\"Ground-truth positives:\", truth_names if truth_names else \"None\")\n",
    "\n",
    "display_images([random_path], [f\"{labels[pred_class]} prediction\"], figsize=(5, 5))\n",
    "\n",
    "plot_scores = concept_scores.cpu().numpy()[::-1]\n",
    "plot_labels = concept_names[::-1]\n",
    "y_pos = np.arange(len(plot_scores))\n",
    "colors = [\"tab:red\" if score >= 0 else \"tab:blue\" for score in plot_scores]\n",
    "fig, ax = plt.subplots(figsize=(10, max(3, 0.4 * len(plot_scores))))\n",
    "ax.barh(y_pos, plot_scores, color=colors, alpha=0.85)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(plot_labels)\n",
    "ax.set_xlabel(\"Concept contribution\")\n",
    "ax.set_title(f\"Contributions for {labels[pred_class]} (prob {prob_value:.3f})\")\n",
    "ax.axvline(0, color=\"gray\", linewidth=0.8)\n",
    "for value, y in zip(plot_scores, y_pos):\n",
    "    offset = 0.01 if value >= 0 else -0.01\n",
    "    ha = \"left\" if value >= 0 else \"right\"\n",
    "    ax.text(value + offset, y, f\"{value:.2f}\", va=\"center\", ha=ha, fontsize=8)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1277128c",
   "metadata": {},
   "source": [
    "## Top-5 Pruning Experiment\n",
    "Prune the final weight matrix `W_g` to keep only the magnitude-top-5 concepts per class, then re-evaluate how often the binarized predictions change (Section 5.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59536d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_W = prune_weights(W, topk=5)\n",
    "pruned_logits = final_logits(activations, mean, std, pruned_W, b)\n",
    "pruned_preds = (torch.sigmoid(pruned_logits) >= 0.5)\n",
    "orig_preds = (probs >= 0.5)\n",
    "diff = (orig_preds != pruned_preds)\n",
    "overall_changed = diff.any(dim=1).float().mean().item() * 100\n",
    "per_class_changed = diff.float().mean(dim=0) * 100\n",
    "print(f\"% of samples with changed decisions after pruning to top-5 concepts: {overall_changed:.2f}%\")\n",
    "for label, pct in zip(labels, per_class_changed):\n",
    "    print(f\"  * {label}: {pct:.2f}% of samples changed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
