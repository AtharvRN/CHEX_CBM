{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VLG-CBM CBL Analysis Notebook\n",
        "Use this notebook to load a trained VLG-CBM run, visualize the learned Concept Bottleneck Layer (CBL), and reproduce the experiments from Sections 5.2â€“5.4 of the paper (top activations, decision interpretability, and NEC pruning). Update `RUN_DIR` below to point to your model artifacts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "1. Load the saved artifacts (`concept_layer.pt`, `W_g.pt`, etc.)\n",
        "2. Recompute concept activations on the validation split and cache image paths\n",
        "3. Plot the top-5 activated images per concept as in Figure 4\n",
        "4. Show the top contributions for a sample decision and compare predictions after pruning to NEC=5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26279c8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "from dataset import CheXpertDataset, get_transforms\n",
        "from vlg_cbm_lib.datasets import BackboneWithConcepts, ConceptLayer\n",
        "from models import get_model, XRV_WEIGHTS\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_backbone_from_config(config, labels, device):\n",
        "    use_xrv_backbone = config[\"backbone\"] in XRV_WEIGHTS\n",
        "    backbone_kwargs = {}\n",
        "    if use_xrv_backbone:\n",
        "        backbone_kwargs[\"target_labels\"] = labels\n",
        "    backbone_model = get_model(\n",
        "        config[\"backbone\"],\n",
        "        num_classes=len(labels),\n",
        "        pretrained=True,\n",
        "        **backbone_kwargs,\n",
        "    )\n",
        "    if config.get(\"backbone_ckpt\"):\n",
        "        import inspect\n",
        "        load_kwargs = {\"map_location\": device}\n",
        "        if \"weights_only\" in inspect.signature(torch.load).parameters:\n",
        "            load_kwargs[\"weights_only\"] = False\n",
        "        ckpt = torch.load(config[\"backbone_ckpt\"], **load_kwargs)\n",
        "        state = ckpt.get(\"model_state_dict\", ckpt)\n",
        "        backbone_model.load_state_dict(state, strict=False)\n",
        "    if config[\"backbone\"] == \"densenet121\":\n",
        "        feature_dim = 1024\n",
        "        backbone = backbone_model.backbone.features\n",
        "    elif config[\"backbone\"] == \"resnet50\":\n",
        "        feature_dim = 2048\n",
        "        backbone = torch.nn.Sequential(*list(backbone_model.backbone.children())[:-1])\n",
        "    else:\n",
        "        feature_dim = getattr(backbone_model, \"feature_dim\", 1024)\n",
        "        class XRVDenseNetBackbone(torch.nn.Module):\n",
        "            def __init__(self, wrapper):\n",
        "                super().__init__()\n",
        "                self.wrapper = wrapper\n",
        "\n",
        "            def forward(self, x):\n",
        "                return self.wrapper.get_features(x)\n",
        "\n",
        "        backbone = XRVDenseNetBackbone(backbone_model)\n",
        "    return backbone.to(device), feature_dim\n",
        "\n",
        "class IndexedCheXpertDataset(CheXpertDataset):\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = super().__getitem__(idx)\n",
        "        return image, label, torch.tensor(idx, dtype=torch.long)\n",
        "\n",
        "def load_run_artifacts(run_dir):\n",
        "    run_dir = Path(run_dir)\n",
        "    config = json.loads(run_dir.joinpath(\"config.json\").read_text())\n",
        "    config[\"output\"] = str(run_dir)\n",
        "    concepts = [line.strip() for line in run_dir.joinpath(\"concepts.txt\").read_text().splitlines() if line.strip()]\n",
        "    labels = config[\"labels\"]\n",
        "    backbone, feature_dim = load_backbone_from_config(config, labels, DEVICE)\n",
        "    concept_layer = ConceptLayer(feature_dim, len(concepts), num_hidden=config.get(\"cbl_hidden_layers\", 1))\n",
        "    ckpt_path = run_dir / \"concept_layer_best.pt\"\n",
        "    if not ckpt_path.exists():\n",
        "        ckpt_path = run_dir / \"concept_layer.pt\"\n",
        "    concept_layer.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    model = BackboneWithConcepts(backbone, concept_layer).to(DEVICE)\n",
        "    model.eval()\n",
        "    W = torch.load(run_dir / \"W_g.pt\")\n",
        "    b = torch.load(run_dir / \"b_g.pt\")\n",
        "    mean = torch.load(run_dir / \"concept_mean.pt\")\n",
        "    std = torch.load(run_dir / \"concept_std.pt\")\n",
        "    return config, concepts, labels, model, W, b, mean, std\n",
        "\n",
        "def make_dataset(config, split=\"valid\", split_csv=None):\n",
        "    data_dir = Path(config[\"data_dir\"])\n",
        "    if split_csv is not None:\n",
        "        csv_path = Path(split_csv)\n",
        "    else:\n",
        "        csv_path = data_dir / f\"{split}.csv\"\n",
        "    img_root = data_dir.parent\n",
        "    dataset = IndexedCheXpertDataset(\n",
        "        csv_path=str(csv_path),\n",
        "        img_root=str(img_root),\n",
        "        transform=get_transforms(224, is_training=False),\n",
        "        labels=config[\"labels\"],\n",
        "        uncertain_strategy=config.get(\"uncertain_strategy\", \"ones\"),\n",
        "        frontal_only=config.get(\"frontal_only\", True)\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def make_loader(dataset, batch_size=64):\n",
        "    def collate(batch):\n",
        "        images, labels, idxs = zip(*batch)\n",
        "        return torch.stack(images), torch.stack(labels), torch.stack(idxs)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate)\n",
        "\n",
        "def compute_activations(model, loader):\n",
        "    model.eval()\n",
        "    all_concepts = []\n",
        "    all_labels = []\n",
        "    all_idxs = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels_batch, idxs in loader:\n",
        "            images = images.to(DEVICE)\n",
        "            logits = model(images).cpu()\n",
        "            all_concepts.append(logits)\n",
        "            all_labels.append(labels_batch)\n",
        "            all_idxs.append(idxs)\n",
        "    return torch.cat(all_concepts), torch.cat(all_labels), torch.cat(all_idxs)\n",
        "\n",
        "def normalize_concepts(concepts, mean, std):\n",
        "    return (concepts - mean) / torch.clamp(std, min=1e-6)\n",
        "\n",
        "def final_logits(concepts, mean, std, W, b):\n",
        "    c_norm = normalize_concepts(concepts, mean, std)\n",
        "    return c_norm @ W.t() + b\n",
        "\n",
        "def compute_contributions(concepts, W):\n",
        "    probs = torch.sigmoid(concepts)\n",
        "    return probs.unsqueeze(1) * W.unsqueeze(0)\n",
        "\n",
        "def prune_weights(W, topk=5):\n",
        "    pruned = torch.zeros_like(W)\n",
        "    abs_W = W.abs()\n",
        "    for class_idx in range(W.size(0)):\n",
        "        topk_idxs = torch.topk(abs_W[class_idx], topk).indices\n",
        "        pruned[class_idx, topk_idxs] = W[class_idx, topk_idxs]\n",
        "    return pruned\n",
        "\n",
        "def display_images(paths, titles=None, figsize=(14, 3)):\n",
        "    n = len(paths)\n",
        "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    for ax, path, title in zip(axes, paths, titles or [None] * n):\n",
        "        ax.imshow(Image.open(path).convert(\"RGB\"))\n",
        "        ax.axis(\"off\")\n",
        "        if title:\n",
        "            ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_top_images_for_concept(concept_idx, activations, idxs, dataset, concept_names, topk=5):\n",
        "    scores = activations[:, concept_idx]\n",
        "    top_indices = torch.argsort(scores, descending=True)[:topk]\n",
        "    paths = [dataset.get_image_path(int(idxs[i])) for i in top_indices]\n",
        "    titles = [f\"{concept_names[concept_idx]} ({scores[i].item():.3f})\" for i in top_indices]\n",
        "    display_images(paths, titles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_DIR = Path(\"saved_models/vlg_cbm_exp5\")  # update to your checkpoint directory\n",
        "config, concepts, labels, model, W, b, mean, std = load_run_artifacts(RUN_DIR)\n",
        "dataset = make_dataset(config, split=\"valid\")\n",
        "loader = make_loader(dataset, batch_size=64)\n",
        "activations, pathology_labels, idxs = compute_activations(model, loader)\n",
        "logits = final_logits(activations, mean, std, W, b)\n",
        "probs = torch.sigmoid(logits)\n",
        "concept_predictions = torch.sigmoid(activations)\n",
        "print(\"Loaded run:\", RUN_DIR)\n",
        "print(\"Activation matrix shape:\", activations.shape)\n",
        "print(\"Dataset size:\", len(dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top-5 Activated Images per Concept\n",
        "Pick a concept of interest (e.g., one that matches VINDr tags) and inspect the five images with the highest CBL activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concept_idx = 0  # update with the concept you want to inspect\n",
        "plot_top_images_for_concept(concept_idx, activations, idxs, dataset, concepts, topk=5)\n",
        "print(\"Concept name:\", concepts[concept_idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Interpretability Case Study\n",
        "Examine the top-5 concept contributions for a validation image by multiplying the concept prediction scores by `W_g`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_contributions(sample_idx, topk=5):\n",
        "    contributions = compute_contributions(activations, W)\n",
        "    sample_probs = probs[sample_idx]\n",
        "    predicted_class = torch.argmax(sample_probs).item()\n",
        "    class_contribs = contributions[sample_idx, predicted_class]\n",
        "    topk_idxs = torch.argsort(class_contribs.abs(), descending=True)[:topk]\n",
        "    return predicted_class, class_contribs[topk_idxs], topk_idxs\n",
        "\n",
        "sample_idx = 0  # change to another sample to inspect\n",
        "pred_class, contribs, contrib_idxs = top_contributions(sample_idx)\n",
        "print(\"Sample image:\", dataset.get_image_path(int(idxs[sample_idx])))\n",
        "display_images([dataset.get_image_path(int(idxs[sample_idx]))], [f\"Predicted class: {labels[pred_class]}\"], figsize=(5, 5))\n",
        "for rank, (idx, score) in enumerate(zip(contrib_idxs, contribs), 1):\n",
        "    print(f\"{rank}. {concepts[idx]} -> contribution {score.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top-5 Pruning Experiment\n",
        "Prune the final weight matrix `W_g` to keep only the magnitude-top-5 concepts per class, then re-evaluate how often the binarized predictions change (Section 5.4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pruned_W = prune_weights(W, topk=5)\n",
        "pruned_logits = final_logits(activations, mean, std, pruned_W, b)\n",
        "pruned_preds = (torch.sigmoid(pruned_logits) >= 0.5)\n",
        "orig_preds = (probs >= 0.5)\n",
        "changed = (orig_preds != pruned_preds).any(dim=1).float().mean().item() * 100\n",
        "print(f\"% of samples with changed decisions after pruning to top-5 concepts: {changed:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
